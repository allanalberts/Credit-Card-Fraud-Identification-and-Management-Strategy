{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone 2 - testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare every classifier using default parameters\n",
    "select best classifiers and tune them\n",
    "try feature engineering for low and high dollar\n",
    "try feature importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# import seaborn as sns\n",
    "import time\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# import sklearn.model_selection as cv\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.ensemble import (GradientBoostingClassifier, \n",
    "                              AdaBoostClassifier,\n",
    "                              RandomForestClassifier)\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "# Measuring Recall & Precision\n",
    "# from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import average_precision_score \n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "import helpers as h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Defaults:\n",
    "SEED=123\n",
    "FOLDS=5\n",
    "CPU=-1\n",
    "\n",
    "# Business Parameters:\n",
    "FraudBudget=0.0005\n",
    "ReviewCost=10\n",
    "ChargebackFee=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"../data/creditcard.csv\")\n",
    "# features = ['V%d' % number for number in range(1, 29)] + ['Amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def feature_engineering(data):\n",
    "#     df = data.copy()\n",
    "#     df['LowAmount'] = (data['Amount'] <= 1)\n",
    "#     df['LowAmount'] = df['LowAmount'].astype(int)\n",
    "#     df['NonLowAmount'] = (data['Amount'] > 1)\n",
    "#     df['NonLowAmount'] = df['NonLowAmount'].astype(int)\n",
    "    \n",
    "#     # since the population mean is zero, I converted the irrelevant features to zero\n",
    "\n",
    "#     df['V15'] = df['LowAmount'] * data['V15']\n",
    "#     df['V24'] = df['LowAmount'] * data['V24']\n",
    "#     df['V26'] = df['LowAmount'] * data['V26']\n",
    "\n",
    "#     df['V8'] = df['NonLowAmount'] * data['V8']\n",
    "#     df['V19'] = df['NonLowAmount'] * data['V19']\n",
    "#     df['V21'] = df['NonLowAmount'] * data['V21']\n",
    "\n",
    "#     feature_lst = ['V%d' % number for number in range(1, 13)] + \\\n",
    "#                   ['V%d' % number for number in range(14, 22)] + \\\n",
    "#                   ['V24', 'V26']\n",
    "#     return df, feature_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_classifier_dict():\n",
    "    \"\"\"\n",
    "    Returns a dictionary of classifier keys include colors for graphing\n",
    "    Dictionary will later be populated with predictions and hyperparameters.\n",
    "    \"\"\"\n",
    "    classifiers = { \n",
    "            \"RF\":  {\"clf_desc\": \"RandomForest\",\n",
    "                    \"model\": RandomForestClassifier(n_jobs=CPU, \n",
    "                                                    random_state=SEED), \n",
    "                    \"c\": \"g\", \n",
    "                    \"cmap\": plt.cm.Greens,\n",
    "                    \"threshold\": 0.5}\n",
    "\n",
    "            ,\"XGB\": {\"clf_desc\": \"XGBoost\",\n",
    "                    \"model\": XGBClassifier(n_jobs=CPU, random_state=SEED), \n",
    "                    \"c\": \"blue\", \n",
    "                    \"cmap\": plt.cm.Blues, \n",
    "                    \"threshold\": 0.5}\n",
    "        \n",
    "            ,\"LR\":  {\"clf_desc\": \"LogisticRegression\",\n",
    "                    \"model\": LogisticRegression(n_jobs=CPU, \n",
    "                                              random_state = SEED), \n",
    "                    \"c\": \"r\", \n",
    "                    \"cmap\": plt.cm.Reds,\n",
    "                    \"threshold\": 0.5}\n",
    "            }\n",
    "    \n",
    "    # Make classifier pipelines:\n",
    "    for clf in classifiers:\n",
    "        steps = [# placeholder for smote: ('Preprocess', FeaturePreprocess), \n",
    "                 ('Classifier', classifiers[clf]['model'])]\n",
    "        classifiers[clf]['pipeline'] = Pipeline(steps)    \n",
    "    \n",
    "    return classifiers\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_classifier(clf):\n",
    "#     '''\n",
    "#     Fits the pipeline stored in classifiers dictionary on X_train, y_train data.\n",
    "#     Makes predictions with pipiline on X_test, y_test data.  \n",
    "#     Results are store in the classifiers dictionary.\n",
    "#     '''     \n",
    "\n",
    "#     model = classifiers[clf]['model'].fit(X_train, y_train)    \n",
    "       \n",
    "#     classifiers[clf][\"y_pred\"] = model.predict(X_test)\n",
    "#     classifiers[clf][\"test_Avg_Pr_score\"] = average_precision_score(y_test, \n",
    "#                                                                classifiers[clf][\"y_pred\"])\n",
    "#     classifiers[clf][\"test_Recall_score\"] = recall_score(y_test, \n",
    "#                                                     classifiers[clf][\"y_pred\"])\n",
    "#     classifiers[clf][\"test_Precision_score\"] = precision_score(y_test, \n",
    "#                                                           classifiers[clf][\"y_pred\"])\n",
    "#     classifiers[clf][\"test_f1_score\"] = f1_score(y_test, \n",
    "#                                             classifiers[clf][\"y_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cvtrain_classifier(clf, X, y):\n",
    "#     '''\n",
    "#     Runs cross validation on the pipeline stored in the classifiers dictionary under subkey['pipeline']\n",
    "#     using X and y data. Results are store in the classifiers dictionary. \n",
    "#     Displays cross validation execution time if TimeIt=True.\n",
    "#     '''     \n",
    "\n",
    "#     cv_scores = cross_validate(classifiers[clf]['model'], X, y,\n",
    "#                                 cv=StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=SEED), \n",
    "#                                 return_train_score=False, \n",
    "#                                 scoring=[\"average_precision\", \"recall\", \"precision\", \"f1_macro\"])\n",
    "\n",
    "#     classifiers[clf][\"cv_Avg_Pr_scores\"] = cv_scores['test_average_precision']\n",
    "#     classifiers[clf][\"cv_Recall_scores\"] = cv_scores['test_recall']\n",
    "#     classifiers[clf][\"cv_Precision_scores\"] = cv_scores['test_precision']         \n",
    "#     classifiers[clf][\"cv_f1_scores\"] = cv_scores['test_f1_macro']   \n",
    "    \n",
    "#     # use average to calculate a singel score:\n",
    "#     classifiers[clf][\"Avg_Pr_score\"] = np.mean(classifiers[clf][\"cv_Avg_Pr_scores\"])\n",
    "#     classifiers[clf][\"Recall_score\"] = np.mean(classifiers[clf][\"cv_Recall_scores\"])\n",
    "#     classifiers[clf][\"Precision_score\"] = np.mean(classifiers[clf][\"cv_Precision_scores\"])\n",
    "#     classifiers[clf][\"f1_score\"] = np.mean(classifiers[clf][\"cv_f1_scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def score_classifiers(clf_lst, TimeIt=True):\n",
    "#     for clf in clf_lst:\n",
    "#         start_time = time.time()\n",
    "\n",
    "#         cvtrain_classifier(clf, X_train, y_train) \n",
    "#         test_classifier(clf)\n",
    "        \n",
    "#         if TimeIt:\n",
    "#             t = time.time() - start_time\n",
    "#             print(f\"{t:.0f} seconds cross_validate execution time for {clf} classifier\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_all_metrics(ax, clf_lst):\n",
    "#     colors = []\n",
    "#     clf_desc = []\n",
    "#     for clf in clf_lst:\n",
    "#         colors.append(classifiers[clf][\"c\"])\n",
    "#         clf_desc.append(classifiers[clf][\"clf_desc\"])\n",
    "#         results = pd.DataFrame.from_dict(classifiers, \n",
    "#                                          orient='index')[[\"clf_desc\", \n",
    "#                                                         \"Recall_score\",\n",
    "#                                                         \"Precision_score\",\n",
    "#                                                         \"f1_score\",\n",
    "#                                                         \"Avg_Pr_score\",]]    \n",
    "#     results.set_index('clf_desc', inplace=True)\n",
    "#     results.T.plot(kind='bar', color=colors, alpha=0.5, rot=0, ax=ax)\n",
    "#     ax.set_ylim(ymin=0, ymax=1.0);\n",
    "#     ax.grid('on', axis='y')\n",
    "#     ax.legend(loc='lower right')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax = plot_all_metrics(ax, clf_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[classifiers[clf][\"cv_Recall_scores\"][0] for clf in classifiers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = gs.GridSpec(1,2)    \n",
    "def plot_fraud_amount_eda(data=data, gs=gs):\n",
    "\n",
    "    fig = plt.figure(figsize=(20,6))\n",
    "    ax1 = fig.add_subplot(gs[0:-1])\n",
    "    ax2 = fig.add_subplot(gs[1:])\n",
    "\n",
    "    \n",
    "grid = plt.GridSpec(2, 4, wspace=0.4, hspace=0.3)\n",
    "plt.subplot(grid[:1, :1])\n",
    "plt.subplot(grid[0, 2])\n",
    "plt.subplot(grid[0, 3])\n",
    "plt.subplot(grid[1, 2])\n",
    "plt.subplot(grid[1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function ROC_curve(probabilities, labels):\n",
    "    Sort instances by their prediction strength (the probabilities)\n",
    "    For every instance in increasing order of probability:\n",
    "        Set the threshold to be the probability\n",
    "        Set everything above the threshold to the positive class\n",
    "        Calculate the True Positive Rate (aka sensitivity or recall)\n",
    "        Calculate the False Positive Rate (1 - specificity)\n",
    "    Return three lists: TPRs, FPRs, thresholds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "probabilities = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "tpr, fpr, thresholds = roc_curve(probabilities, y_test)\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n",
    "plt.ylabel(\"True Positive Rate (Sensitivity, Recall)\")\n",
    "plt.title(\"ROC plot of Loan Interest Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ISOLATION FOREST\n",
    "If instances return a score very close to 1, then they are definitely anomalies.\n",
    "If instances have a score much smaller than 0.5, then they are quite safe to be regarded as normal instances.\n",
    "If all the instances return a score ≈ 0.5, then the entire sample does not really have any distinct anomaly.\n",
    "\n",
    "You can set the contamination parameter to whatever percent your heart desires as long as it's a float in (0., 0.5).\n",
    "\"The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function.\"\n",
    "The default is 0.1 (or 10%), so you could set contamination=0.04 (4%).\n",
    "from sklearn.ensemble import IsolationForest\n",
    "clf = IsolationForest(contamination=0.04)\n",
    "\n",
    "<!-- # Isolation Forest is unsupervised - just check it against the test set\n",
    "#         The distance of the path is averaged and normalised to calculate \n",
    "#         the anomaly score. Anomaly score of 1 is considered as an outlier, \n",
    "#         values close to 0 is considered normal. -->\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
